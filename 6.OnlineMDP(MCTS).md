# 10 - Online MDP Planning with Monte Carlo Tree Search
Balancing Exploitation and Exploration

## Offline Planning vs Online Planning
+ Offline Planning
    * value iteration and policy iteration for MDPs are Offline Planning
    * state space _S_ usually far too big
        - ~~poker machine too big to put in MDP~~
        - ~~can approximate the MDP by reducing the dimensionality of _S_ but not discussed in this subject~~
+ Online Planning
    * actions selected online at each state during execution
        - for each visited state, evaluate many policies _π_ (partially)
        - the quality of each π̂ is approximated by averaging the expected reward of trajectories over _S_ obtained by repeated simulations of `r(s, a, s0)`
            + how to do repeated simulations
                * __Monte Carlo__ methods are by far the most popular
                    - get the name by using randomness (chance) to search the action space, like gambling/casinos
        - select the best _π̂_ and execute the action _π̂(s)_

## Monte Carlo Tree Search
+ invoked every time it visits a new state _s_
    * fundamental features
        - The _Q-value_ for each action _a_ at state _s_, `Q(s, a)`, is approximated using random simulation.
        - These estimates of `Q(s, a)` are used to drive a __best-first__ strategy; thus, `Q(s, a)` is being updated while it is also be used as an heuristic in the search.
        - ~~A search tree is built incrementally.~~
        - ~~The search terminates when some pre-defined computational budget is used up, such as a time limit or a number of expanded nodes. Therefore, it is an anytime algorithm, as it can be terminated at any time and still give an answer.~~
        - ~~The best performing action _a_ at _s_ is returned; that is `argmax_{a∈A}{Q(s, a)}`.~~
        - ~~This is complete if there are no dead–ends.~~
        - ~~This is optimal iif the computational budget is sufficient, and is perfect if an entire search can be performed (which is unusual).~~
+ Nodes store tuple <_s_, _a_, pointer to parent, `Q(a, s)`>
    * note that _a_ is the action which is going to be applied on _s_
+ the Algorithm
    * Sketch
        - The evaluated states are stored in a search tree. The set of evaluated states is incrementally built by __iterating__ over the following four steps (expansion and simulation are somehow hybrid):
            + Select
                * ~~Given a tree policy, select a single node in the tree to assess~~
                * happens in __TreePolicy(root)__
                    - Select _recursively_ the most __promising__ node (s, a) to expand
                        + __best-first__
                    - Check if the generated nodes (s0 , b) are already in tree
                    - If not in the search tree, add these nodes to the tree.
                    - Return last node as best
            + Expand
                * ~~Expand this node by applying one available action~~ (actually the corresponding action stored in the tuple)
                    - simulate to get `s-next`
                    - then create nodes for pairs `(s-next , b)`
                        + _b_ ∈ _A_ and executable on _s-next_
                        + would expand the corresponding state for all available actions
                * happens in __DefaultPolicy(best)__
                    - Arbitrary policy _π(s0)_ used to evaluate below node _(s, a)_
                    - ∆ is the maximal reward attained by _π(s0)_
                    - ~~In some applications _π(s0)_ is hand-coded~~
                    - ~~Domain independent version of _π(s0)_: random walk, simulated annealing, ...~~
                    - Think of __DefaultPolicy__ as a __heuristic__
            + Simulate
                * From the expanded node, perform a complete random simulation to a leaf node, with probability `P_a{(s, s-next)}` (execute action _a_ from the MDP model in state _s_, choosing state _s-next_ with probability `P_a(s, s-next)`). ~~This therefore assumes that the search tree is finite (but version for infinitely large trees exist).~~
                    - `P_a(s, s0)` is stochastic, so several visits (in theory an infinite number) may be necessary to generate all successors.
                * happens in __DefaultPolicy(best)__
                    - Arbitrary policy _π(s0)_ used to evaluate below node _(s, a)_
                    - ∆ is the maximal reward attained by π(s0)
                    - ~~In some applications _π(s0)_ is hand-coded~~
                    - ~~Domain independent version of _π(s0)_: random walk, simulated annealing, ...~~
                    - Think of __DefaultPolicy__ as a __heuristic__
            + Backpropagate
                * ~~Finally, the value of the node is backpropagated to the root node, updating the value of each ancestor node on the way.~~
                * happens in __Backup(best, ∆)__
                    - ∆ is backpropagated from best to their parents recursively.
                    - a discount factor can be used, as in value iteration, policy iteration, etc.











+ e-greedy
+ e-decreasing
+ UCB