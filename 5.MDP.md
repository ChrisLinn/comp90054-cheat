# MDP, Markov Decision Process

## Classical planning
+ deterministic action -> non-deterministic (but with probability)
+ static environment
    * Environments change only as the result of an action
+ perfect knowledge (full observability)
    * omniscience
+ single actor
    * omnipotence

## Markov Decision Process (MDP) 
The most common formulation of MDPs is a Discounted-Reward Markov Decision Process. Optimal solutions maximise the expected discounted __accumulated__ reward from the initial state _s0_.

+ transition probability
+ policy, π
    * mapping from states to actions
    * 不像 classical 里面返回 action sequence, policy 对于每一个 state 都返回一个 action
        - 因为 non-deterministic
        - classical 算 heuristic 的话，不用全 states 都算，MDP 要全算但算好了就有所有 states 的
+ reward `r(s, a, s0)` 
    * There are no action costs. These are modelled as negative rewards.
    * There are no goals. Each action receives a reward when applied. The value of the reward is dependent on the state in which it is applied.
+ a discount factor (can be iteratively defined)
    * `0 ≤ γ ≤ 1`
+ how to solve MDP
    * two common ways
        - value iteration
        - policy iteration
    
## Value Iteration
* V(s)
    - Bellman equation
    ``` 
        max(a in actions)
            (sum(all states)
                (P(s,a,s')*(reward(s,a,s')+γV(s'))))
    ```
    - = `max(a in actions)(Q(s, a))`
        + Q-value: `Q(s, a)` 
            * 
            ``` 
                sum(all states)
                    (P(s,a,s')*(reward(s,a,s')+γV(s')))
            ```
            * 针对不同的 action
            * 一个 action 可能去到不同的 state
                - P, reward 都要针对 state+action 才行
                - 所以 `sum(all states)`
        + converges exponentially fast to the optimal policy
            * can be easily parallelised
    - policy extraction
        + `argmax`

## policy iteration
* `V^{π}(s)`
    - expected discounted __accumulated__ reward/cost of following the policy π from _s_
    - weighted avg of cost of the possible state sequences 
* `Q^{π}(s,a)`
