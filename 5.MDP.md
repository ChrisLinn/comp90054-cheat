# classical planning
+ deterministic action -> non-deterministic (but with probability)
+ static environment
+ perfect knowledge (full observabiliyu)
+ single actor

---
->
Markov Decision Process (MDP)

+ transition probability
+ reward
+ a discount factor (can be iteratively defined)
+ value
+ Q
+ policy
    * mapping from states to actions
    * 不像 classical 里面返回 action seq, policy 对于每一个 state 都返回一个 action
        - 因为 non-deterministic
+ heuristic 的话，不用全 states 都算，MDP 要全算但算好了就有所有 states 的
