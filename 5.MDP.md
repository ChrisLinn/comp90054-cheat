# MDP, Markov Decision Process

## Classical planning
+ deterministic action -> non-deterministic (but with probability)
+ static environment
+ perfect knowledge (full observability)
+ single actor

## Markov Decision Process (MDP)

+ transition probability
+ reward
+ a discount factor (can be iteratively defined)
+ value
+ Q(s, a)
    * 
    ``` 
        sum(all states)
            (P(s,a,s')*(reward(s,a,s')+γV(s')))
    ```
    * 针对不同的 action
    * 一个 action 可能去到不同的 state
        - P, reward 都要针对 state+action 才行
        - 所以 sum(all states)
+ V(s)
    * = max(a in actions)(Q(s, a))
    * 
    ``` 
    max(a in actions)
        (sum(all states)
            (P(s,a,s')*(reward(s,a,s')+γV(s'))))
    ```
+ policy, π
    * mapping from states to actions
    * 不像 classical 里面返回 action seq, policy 对于每一个 state 都返回一个 action
        - 因为 non-deterministic
        - classical 算 heuristic 的话，不用全 states 都算，MDP 要全算但算好了就有所有 states 的
+ V^{π}(s)
    * expected __cost__ of policy
    * weighted avg of cost of the possible state sequences 
+ Q^{π}(s,a)
