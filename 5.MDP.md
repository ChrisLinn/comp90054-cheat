# MDP, Markov Decision Process

## Classical planning
+ deterministic action -> non-deterministic (but with probability)
+ static environment
    * Environments change only as the result of an action
+ perfect knowledge (full observability)
    * omniscience
+ single actor
    * omnipotence

## Markov Decision Process (MDP) 
The most common formulation of MDPs is a Discounted-Reward Markov Decision Process.

+ transition probability
+ reward
    * There are no action costs. These are modelled as negative rewards.
    * There are no goals. Each action receives a reward when applied. The value of the reward is dependent on the state in which it is applied.
+ a discount factor (can be iteratively defined)
+ value
+ Q(s, a)
    * 
    ``` 
    sum(all states)
        (P(s,a,s')*(reward(s,a,s')+γV(s')))
    ```
    * 针对不同的 action
    * 一个 action 可能去到不同的 state
        - P, reward 都要针对 state+action 才行
        - 所以 sum(all states)
+ V(s)
    * = max(a in actions)(Q(s, a))
    * 
    ``` 
    max(a in actions)
        (sum(all states)
            (P(s,a,s')*(reward(s,a,s')+γV(s'))))
    ```
+ policy, π
    * mapping from states to actions
    * 不像 classical 里面返回 action seq, policy 对于每一个 state 都返回一个 action
        - 因为 non-deterministic
        - classical 算 heuristic 的话，不用全 states 都算，MDP 要全算但算好了就有所有 states 的
+ V^{π}(s)
    * expected __cost__ of policy, expected discounted reward from _s_
    * weighted avg of cost of the possible state sequences 
+ Q^{π}(s,a)
